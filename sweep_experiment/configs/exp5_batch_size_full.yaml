# Series 39: Experiment 5 -- Retrieval-Augmented Batch-Size Ablation (Full-model)
# Goal: Study impact of retrieval-augmented batch-level TTA vs instance-level.
# Eval set: same 100 videos from --data-dir
# Retrieval pool: 1000-video superset in retrieval_pool_dir
# Fixed: Full-model best config (lr=1e-5, steps=20, optimizer=sgd)
# Swept: batch_videos
# NOTE: batch support implemented in run_full_tta.py (--retrieval-pool-dir, --batch-videos)
#
# COST & TRAINING NOTE:
#   - Pre-encoding time scales linearly with K (each neighbour needs VAE + text encode).
#   - Training time is constant (num_steps is fixed regardless of K).
#   - BUT training steps are distributed round-robin, so each video gets ~num_steps/K
#     gradient updates. At K=100, steps=20, the eval video is only trained on ~once.
#   - If results show under-training at large K, consider a follow-up sweep that
#     increases num_steps proportionally with K (e.g. steps=20*K or steps=max(20, K)).

method: full
series: 39
series_name: exp5_batch_size_full
description: "Retrieval-augmented batch-size ablation: Full-model TTA"

fixed:
  learning_rate: 1.0e-5
  num_steps: 20
  warmup_steps: 3
  weight_decay: 0.01
  max_grad_norm: 1.0
  optimizer: sgd
  batch_method: similarity
  retrieval_pool_dir: /scratch/wc3013/longcat-video-tta/datasets/panda_1000_480p
  num_cond_frames: 14
  num_frames: 28
  gen_start_frame: 32
  num_inference_steps: 50
  guidance_scale: 4.0
  resolution: 480p
  seed: 42
  max_videos: 100

sweep:
  - run_id: BS_F1
    batch_videos: 1
  - run_id: BS_F2
    batch_videos: 5
  - run_id: BS_F3
    batch_videos: 10
  - run_id: BS_F4
    batch_videos: 50
  - run_id: BS_F5
    batch_videos: 100
