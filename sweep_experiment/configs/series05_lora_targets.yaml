# Series 5: LoRA -- Target Module Scope
# Goal: Determine if adding FFN layers helps or hurts TTA.
# Fixed: lora_rank=BEST, lr=BEST, steps=20
# Swept: target_ffn
# NOTE: Update lora_rank, lora_alpha, learning_rate after Series 3+4.

method: lora
series: 5
series_name: lora_target_sweep
description: "LoRA target module scope (attn-only vs attn+FFN)"
depends_on: [3, 4]

fixed:
  lora_rank: 8        # PLACEHOLDER -- update from Series 3 best
  lora_alpha: 16      # PLACEHOLDER -- 2x rank
  learning_rate: 2.0e-4  # PLACEHOLDER -- update from Series 4 best
  num_steps: 20
  warmup_steps: 3
  weight_decay: 0.01
  max_grad_norm: 1.0
  target_modules: "qkv,proj"
  num_cond_frames: 2
  num_frames: 16
  gen_start_frame: 32
  num_inference_steps: 50
  guidance_scale: 4.0
  resolution: 480p
  seed: 42
  max_videos: 100

sweep:
  - run_id: L11
    target_ffn: false
  - run_id: L12
    target_ffn: true
